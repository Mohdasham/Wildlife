# -*- coding: utf-8 -*-
"""main.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mLRxupWAnVQa_NPsT0bqlOp5y0hy1s7Y
"""

from git import Repo

repo_url = "https://github.com/xiaohuicui/WAID.git"
local_dir = "/path/to/your/local/directory"  # Update this path
Repo.clone_from(repo_url, local_dir)

import os
import shutil
import yaml

class WildlifeDatasetOrganizer:
    def __init__(self, base_images_dir, base_labels_dir, output_dir):
        """
        Initialize dataset organizer

        :param base_images_dir: Base directory containing train/test/valid image folders
        :param base_labels_dir: Base directory containing train/test/valid label folders
        :param output_dir: Output directory for organized dataset
        """
        self.base_images_dir = base_images_dir
        self.base_labels_dir = base_labels_dir
        self.output_dir = output_dir

        # Predefined classes
        self.classes = ['sheep', 'cattle', 'seal', 'camelus', 'kiang', 'zebra']
        self.class_to_index = {cls: idx for idx, cls in enumerate(self.classes)}

        # Create output directories
        self._create_output_directories()

    def _create_output_directories(self):
        """Create output directories for YOLO format"""
        splits = ['train', 'val', 'test']
        for split in splits:
            os.makedirs(os.path.join(self.output_dir, 'images', split), exist_ok=True)
            os.makedirs(os.path.join(self.output_dir, 'labels', split), exist_ok=True)

    def organize_dataset(self):
        """
        Organize dataset into YOLO format
        """
        splits = ['train', 'test', 'val']

        for split in splits:
            # Paths for current split
            images_split_dir = os.path.join(self.base_images_dir, split)
            labels_split_dir = os.path.join(self.base_labels_dir, split)

            # Skip if directory doesn't exist
            if not os.path.exists(images_split_dir) or not os.path.exists(labels_split_dir):
                print(f"Warning: {split} directory not found. Skipping.")
                continue

            # Output paths
            output_images_dir = os.path.join(self.output_dir, 'images', split)
            output_labels_dir = os.path.join(self.output_dir, 'labels', split)

            # Process each image in the split
            for image_filename in os.listdir(images_split_dir):
                if not image_filename.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.tiff')):
                    continue

                # Construct full paths
                image_path = os.path.join(images_split_dir, image_filename)
                base_filename = os.path.splitext(image_filename)[0]

                # Find corresponding label file
                label_filename = base_filename + '.txt'
                label_path = os.path.join(labels_split_dir, label_filename)

                # Skip if no corresponding label file
                if not os.path.exists(label_path):
                    print(f"Warning: No label file found for {image_filename}")
                    continue

                # Copy image to output directory
                shutil.copy(image_path, os.path.join(output_images_dir, image_filename))

                # Copy label file to output directory
                shutil.copy(label_path, os.path.join(output_labels_dir, label_filename))

            print(f"Processed {split} split")

        # Create YOLO dataset configuration
        self._create_dataset_yaml()

    def _create_dataset_yaml(self):
        """
        Create YAML configuration file for YOLO
        """
        data = {
            'train': os.path.join(self.output_dir, 'images', 'train'),
            'val': os.path.join(self.output_dir, 'images', 'val'),
            'test': os.path.join(self.output_dir, 'images', 'test'),
            'nc': len(self.classes),
            'names': self.classes
        }

        yaml_path = os.path.join(self.output_dir, 'wildlife.yaml')
        with open(yaml_path, 'w') as f:
            yaml.dump(data, f, default_flow_style=False)

        print(f"Dataset YAML created at {yaml_path}")

    def analyze_dataset(self):
        """
        Analyze dataset composition
        """
        splits = ['train', 'test', 'val']
        dataset_stats = {}

        for split in splits:
            images_dir = os.path.join(self.output_dir, 'images', split)
            labels_dir = os.path.join(self.output_dir, 'labels', split)

            # Skip if directory doesn't exist
            if not os.path.exists(images_dir) or not os.path.exists(labels_dir):
                print(f"Warning: {split} directory not found. Skipping analysis.")
                continue

            # Count images and annotations
            images = [f for f in os.listdir(images_dir) if f.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.tiff'))]
            labels = [f for f in os.listdir(labels_dir) if f.endswith('.txt')]

            # Analyze class distribution
            class_counts = {cls: 0 for cls in self.classes}
            for label_file in labels:
                with open(os.path.join(labels_dir, label_file), 'r') as f:
                    for line in f:
                        try:
                            class_index = int(line.split()[0])
                            if 0 <= class_index < len(self.classes):  # Added bounds check
                                class_counts[self.classes[class_index]] += 1
                            else:
                                print(f"Warning: Invalid class index {class_index} in {label_file}")
                        except (IndexError, ValueError) as e:
                            print(f"Warning: Invalid label format in {label_file}: {e}")

            dataset_stats[split] = {
                'total_images': len(images),
                'total_labels': len(labels),
                'class_distribution': class_counts
            }

        # Print dataset statistics
        print("\nDataset Statistics:")
        for split, stats in dataset_stats.items():
            print(f"\n{split.upper()} Split:")
            print(f"Total Images: {stats['total_images']}")
            print(f"Total Labeled Objects: {sum(stats['class_distribution'].values())}")
            print("Class Distribution:")
            for cls, count in stats['class_distribution'].items():
                print(f"  {cls}: {count}")

        return dataset_stats

class WildlifeDataset(Dataset):
    def __init__(self, images_dir, labels_dir, transform=None):
        """
        Custom PyTorch Dataset for wildlife images

        :param images_dir: Directory containing images
        :param labels_dir: Directory containing label files
        :param transform: Optional image transformations
        """
        self.images_dir = images_dir
        self.labels_dir = labels_dir
        self.transform = transform

        # Find all image files
        self.image_files = [f for f in os.listdir(images_dir)
                            if f.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp'))]

    def __len__(self):
        return len(self.image_files)

    def __getitem__(self, idx):
        # Get image filename
        img_filename = self.image_files[idx]
        img_path = os.path.join(self.images_dir, img_filename)

        # Read image
        img = cv2.imread(img_path)
        if img is None:
            raise ValueError(f"Failed to load image: {img_path}")

        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

        # Resize with consistent method
        img = cv2.resize(img, (640, 640), interpolation=cv2.INTER_AREA)

        # Convert to float and normalize
        img = img.astype(np.float32) / 255.0

        # Apply transformations if any
        if self.transform:
            transformed = self.transform(image=img)
            img = transformed['image']

        # Convert to tensor with explicit dtype
        img_tensor = torch.from_numpy(img).permute(2, 0, 1).float()

        # Read corresponding label
        label_filename = os.path.splitext(img_filename)[0] + '.txt'
        label_path = os.path.join(self.labels_dir, label_filename)

        # Read labels with error handling
        labels = torch.tensor([], dtype=torch.float32)
        if os.path.exists(label_path):
            try:
                with open(label_path, 'r') as f:
                    labels_list = [list(map(float, line.strip().split())) for line in f]
                if labels_list:  # Check if the list is not empty
                    labels = torch.tensor(labels_list, dtype=torch.float32)
            except Exception as e:
                print(f"Error reading labels for {img_filename}: {e}")

        return img_tensor, labels

class WildlifeDetectionPipeline:
    def __init__(self, dataset_path, batch_size=8, num_workers=4):
        """
        Initialize Wildlife Detection Pipeline

        :param dataset_path: Path to prepared YOLO dataset
        :param batch_size: Batch size for data loading
        :param num_workers: Number of workers for data loading
        """
        self.dataset_path = dataset_path
        self.batch_size = batch_size
        self.num_workers = num_workers

        # Align with the first script's class list
        self.classes = ['sheep', 'cattle', 'seal', 'camelus', 'kiang', 'zebra']

        # Load dataset configuration
        yaml_path = os.path.join(self.dataset_path, 'wildlife.yaml')
        if not os.path.exists(yaml_path):
            raise FileNotFoundError(f"Dataset configuration file not found at {yaml_path}")

        with open(yaml_path, 'r') as f:
            self.dataset_config = yaml.safe_load(f)

    def _create_dataloader(self, images_dir, labels_dir, shuffle=True):
        """
        Create a DataLoader for a specific dataset split

        :param images_dir: Directory containing images
        :param labels_dir: Directory containing labels
        :param shuffle: Whether to shuffle the data
        :return: PyTorch DataLoader
        """
        # Check if directories exist
        if not os.path.exists(images_dir):
            raise FileNotFoundError(f"Images directory not found: {images_dir}")
        if not os.path.exists(labels_dir):
            raise FileNotFoundError(f"Labels directory not found: {labels_dir}")

        # Basic augmentation with Albumentations
        transform = A.Compose([
            A.RandomCrop(width=640, height=640),
            A.HorizontalFlip(p=0.5),
            A.RandomBrightnessContrast(p=0.2),
        ])

        dataset = WildlifeDataset(images_dir, labels_dir, transform=transform)

        # Custom collate function to handle variable-length labels
        def custom_collate(batch):
            images = torch.stack([item[0] for item in batch])

            # Handle labels
            labels = [item[1] for item in batch]

            # Return images and original (non-padded) labels
            return images, labels

        return DataLoader(
            dataset,
            batch_size=self.batch_size,
            shuffle=shuffle,
            num_workers=self.num_workers,
            pin_memory=True,
            collate_fn=custom_collate
        )

    def train_and_evaluate_models(self):
        """
        Train and evaluate models using memory-efficient approach

        :return: Performance metrics
        """
        # Create dataloaders
        try:
            # Corrected paths: labels directory must be specified correctly
            train_images_dir = self.dataset_config['train']
            train_labels_dir = os.path.join(self.dataset_path, 'labels', 'train')

            val_images_dir = self.dataset_config['val']
            val_labels_dir = os.path.join(self.dataset_path, 'labels', 'val')

            train_loader = self._create_dataloader(train_images_dir, train_labels_dir)
            val_loader = self._create_dataloader(val_images_dir, val_labels_dir, shuffle=False)

            # Process and log dataset information
            print(f"Training samples: {len(train_loader.dataset)}")
            print(f"Validation samples: {len(val_loader.dataset)}")

            # Simulate model training with memory-safe iterations
            self._simulate_training(train_loader, val_loader)

        except Exception as e:
            print(f"Error in data loading: {e}")
            import traceback
            traceback.print_exc()

    def _simulate_training(self, train_loader, val_loader):
        """
        Simulate training process with memory-safe iterations

        :param train_loader: Training data loader
        :param val_loader: Validation data loader
        """
        # Basic training simulation
        for epoch in range(3):  # Simulate 3 epochs
            print(f"\nEpoch {epoch + 1}")

            # Training phase
            for batch_idx, (images, labels) in enumerate(train_loader):
                # Move to device if using GPU
                images = images.float()

                # Print batch information
                print(f"Batch {batch_idx + 1}/{len(train_loader)}")
                print(f"  Images shape: {images.shape}")
                print(f"  Number of labels: {len(labels)}")

                # Clear GPU cache periodically
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()

                # Break after a few batches to prevent long-running simulation
                if batch_idx >= 5:
                    break

class ModelComparisonPipeline:
    def __init__(self, dataset_path):
        """
        Initialize Model Comparison Pipeline

        :param dataset_path: Path to prepared YOLO dataset
        """
        self.dataset_path = dataset_path
        self.classes = ['sheep', 'cattle', 'seal', 'camelus', 'kiang', 'zebra']

        # Required imports for the neural network models
        from tensorflow.keras.models import Sequential, Model
        from tensorflow.keras.layers import Dense, Conv2D, Flatten, Dropout, BatchNormalization
        from tensorflow.keras.layers import MaxPooling2D, LeakyReLU, Input, Concatenate, GlobalAveragePooling2D
        from tensorflow.keras import applications

        # Model configurations with implementations
        self.models = {
            'YOLOv5': {
                'description': 'YOLOv5 with CSP backbone and PANet neck',
                'approach': 'Single-stage detector with CSP networks'
            },
            'YOLOv8': {
                'description': 'YOLOv8 with C2f blocks and improved head',
                'approach': 'Anchor-free detection with modified C2f architecture'
            },
            'RCNN': {
                'description': 'Faster R-CNN with ResNet50 backbone',
                'approach': 'Two-stage detector with region proposal network'
            }
        }

    def prepare_dataset(self, with_smote=False):
        """
        Prepare dataset with optional SMOTE oversampling

        :param with_smote: Whether to apply SMOTE
        :return: Prepared dataset
        """
        # Load images and labels
        train_images_dir = os.path.join(self.dataset_path, 'images', 'train')
        train_labels_dir = os.path.join(self.dataset_path, 'labels', 'train')

        # Check if directories exist
        if not os.path.exists(train_images_dir):
            raise FileNotFoundError(f"Images directory not found: {train_images_dir}")
        if not os.path.exists(train_labels_dir):
            raise FileNotFoundError(f"Labels directory not found: {train_labels_dir}")

        # Collect image features and labels
        X, y = [], []

        # Increase the number of samples for better SMOTE application
        max_samples = 500
        sample_count = 0

        for img_file in os.listdir(train_images_dir):
            if not img_file.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp')):
                continue

            if sample_count >= max_samples:
                break

            try:
                # Read image
                img_path = os.path.join(train_images_dir, img_file)
                img = cv2.imread(img_path)
                if img is None:
                    print(f"Warning: Could not load image {img_path}")
                    continue

                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
                img = cv2.resize(img, (224, 224))

                # Flatten and normalize image (use reduced dimensions for memory efficiency)
                img_reduced = cv2.resize(img, (64, 64))  # Reduced size
                img_flat = img_reduced.flatten().astype(np.float32)

                # Read corresponding label
                label_file = os.path.splitext(img_file)[0] + '.txt'
                label_path = os.path.join(train_labels_dir, label_file)

                if not os.path.exists(label_path):
                    continue

                # Extract class from first line of label file
                with open(label_path, 'r') as f:
                    first_line = f.readline().strip().split()
                    if not first_line:
                        continue
                    class_index = int(first_line[0])

                    # Validate class index
                    if class_index < 0 or class_index >= len(self.classes):
                        print(f"Warning: Invalid class index {class_index} in {label_file}")
                        continue

                # Add to dataset
                X.append(img_flat)
                y.append(class_index)
                sample_count += 1

            except Exception as e:
                print(f"Error processing {img_file}: {e}")

        # Check if we have enough data
        if len(X) < 10:
            raise ValueError("Not enough valid samples found. Check your dataset.")

        # Convert to numpy arrays
        X = np.array(X)
        y = np.array(y)

        # Normalize features
        scaler = StandardScaler()
        X = scaler.fit_transform(X)

        # Apply SMOTE if requested and if we have enough samples of each class
        if with_smote:
            try:
                # Count samples per class
                class_counts = np.bincount(y)
                print(f"Class distribution before SMOTE: {class_counts}")

                # Reduce n_neighbors for SMOTE to work with small classes
                min_samples = min(class_counts[class_counts > 0])
                if min_samples < 6:
                    k_neighbors = min(min_samples - 1, 5)  # Must be less than min_samples
                    if k_neighbors < 1:
                        print("Not enough samples in minority class for SMOTE, skipping SMOTE")
                    else:
                        print(f"Using k_neighbors={k_neighbors} for SMOTE")
                        smote = SMOTE(random_state=42, k_neighbors=k_neighbors)
                        X, y = smote.fit_resample(X, y)
                        print(f"Class distribution after SMOTE: {np.bincount(y)}")
                else:
                    # Use default SMOTE
                    smote = SMOTE(random_state=42)
                    X, y = smote.fit_resample(X, y)
                    print(f"Class distribution after SMOTE: {np.bincount(y)}")

            except Exception as e:
                print(f"Error applying SMOTE: {e}. Continuing without SMOTE.")

        return X, y

    def evaluate_model(self, X, y, model_name):
        """
        Evaluate model performance based on the specified model architecture

        :param X: Input features
        :param y: Labels
        :param model_name: Name of the model
        :return: Performance metrics
        """
        # Reduce folds if data is limited
        n_splits = min(5, len(np.unique(y)))
        kf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)

        # Initialize performance metrics
        precisions, recalls, f1_scores, maps = [], [], [], []

        for train_index, test_index in kf.split(X, y):
            # Split data
            X_train, X_test = X[train_index], X[test_index]
            y_train, y_test = y[train_index], y[test_index]

            # Train model based on model_name with proper architecture approach
            if model_name == 'YOLOv5':
                # YOLOv5 approach - Uses CSP backbone and PANet neck
                model = Sequential([
                    # CSP-like feature extraction (simplified version)
                    Conv2D(32, (3, 3), padding='same', input_shape=(64, 64, 3)), # Adjust for flattened input
                    BatchNormalization(),
                    LeakyReLU(alpha=0.1),
                    MaxPooling2D(pool_size=(2, 2)),

                    # Add depth (CSP-inspired)
                    Conv2D(64, (3, 3), padding='same'),
                    BatchNormalization(),
                    LeakyReLU(alpha=0.1),
                    MaxPooling2D(pool_size=(2, 2)),

                    # Flatten for classification
                    Flatten(),
                    Dense(128, activation='relu'),
                    Dropout(0.5),
                    Dense(len(self.classes), activation='softmax')
                ])
                model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

                # Reshape X_train and X_test to match input shape expected by CNN
                X_train_reshaped = X_train.reshape(-1, 64, 64, 3)
                X_test_reshaped = X_test.reshape(-1, 64, 64, 3)

                # Train model
                model.fit(X_train_reshaped, y_train, epochs=5, batch_size=32, verbose=0)

                # Predict
                y_pred_proba = model.predict(X_test_reshaped)
                y_pred = np.argmax(y_pred_proba, axis=1)

            elif model_name == 'YOLOv8':
                # YOLOv8 approach - Uses C2f blocks and improved head
                inputs = Input(shape=(64, 64, 3))

                # First conv block
                x = Conv2D(32, (3, 3), padding='same')(inputs)
                x = BatchNormalization()(x)
                x = LeakyReLU(alpha=0.1)(x)
                x = MaxPooling2D(pool_size=(2, 2))(x)

                # C2f-like block (simplified)
                # Branch 1
                branch_1 = Conv2D(32, (1, 1), padding='same')(x)
                branch_1 = BatchNormalization()(branch_1)
                branch_1 = LeakyReLU(alpha=0.1)(branch_1)

                # Branch 2
                branch_2 = Conv2D(32, (1, 1), padding='same')(x)
                branch_2 = BatchNormalization()(branch_2)
                branch_2 = LeakyReLU(alpha=0.1)(branch_2)
                branch_2 = Conv2D(32, (3, 3), padding='same')(branch_2)
                branch_2 = BatchNormalization()(branch_2)
                branch_2 = LeakyReLU(alpha=0.1)(branch_2)

                # Concat branches (C2f style)
                x = Concatenate()([branch_1, branch_2])
                x = Conv2D(64, (1, 1), padding='same')(x)
                x = BatchNormalization()(x)
                x = LeakyReLU(alpha=0.1)(x)
                x = MaxPooling2D(pool_size=(2, 2))(x)

                # Flatten and classification head
                x = Flatten()(x)
                x = Dense(128, activation='relu')(x)
                x = Dropout(0.5)(x)
                outputs = Dense(len(self.classes), activation='softmax')(x)

                model = Model(inputs=inputs, outputs=outputs)
                model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

                # Reshape X_train and X_test
                X_train_reshaped = X_train.reshape(-1, 64, 64, 3)
                X_test_reshaped = X_test.reshape(-1, 64, 64, 3)

                # Train model
                model.fit(X_train_reshaped, y_train, epochs=5, batch_size=32, verbose=0)

                # Predict
                y_pred_proba = model.predict(X_test_reshaped)
                y_pred = np.argmax(y_pred_proba, axis=1)

            elif model_name == 'RCNN':
                # Faster R-CNN approach - Uses two-stage detection with RPN
                # For simplified implementation, using backbone feature extractor only
                backbone = applications.ResNet50(
                    include_top=False,
                    weights=None,
                    input_shape=(64, 64, 3)
                )

                # Add classification head
                model = Sequential([
                    backbone,
                    GlobalAveragePooling2D(),
                    Dense(256, activation='relu'),
                    Dropout(0.5),
                    Dense(len(self.classes), activation='softmax')
                ])

                model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

                # Reshape X_train and X_test
                X_train_reshaped = X_train.reshape(-1, 64, 64, 3)
                X_test_reshaped = X_test.reshape(-1, 64, 64, 3)

                # Train model
                model.fit(X_train_reshaped, y_train, epochs=5, batch_size=16, verbose=0)

                # Predict
                y_pred_proba = model.predict(X_test_reshaped)
                y_pred = np.argmax(y_pred_proba, axis=1)

            # Calculate metrics
            precisions.append(precision_score(y_test, y_pred, average='weighted', zero_division=0))
            recalls.append(recall_score(y_test, y_pred, average='weighted', zero_division=0))
            f1_scores.append(f1_score(y_test, y_pred, average='weighted', zero_division=0))

            # Create one-hot encoding for y_test for mAP calculation
            y_test_one_hot = np.zeros((len(y_test), len(self.classes)))
            for i, label in enumerate(y_test):
                if 0 <= label < len(self.classes):
                    y_test_one_hot[i, label] = 1

            try:
                map_score = average_precision_score(
                    y_test_one_hot,
                    y_pred_proba,
                    average='weighted'
                )
                maps.append(map_score)
            except Exception as e:
                print(f"Error calculating mAP: {e}")
                maps.append(0)

        # Return average metrics
        return {
            'precision': np.mean(precisions),
            'recall': np.mean(recalls),
            'f1_score': np.mean(f1_scores),
            'mAP': np.mean(maps)
        }

    def compare_models(self):
        """
        Compare models with and without SMOTE

        :return: Comparative performance results
        """
        results = {}

        # Scenarios to evaluate
        scenarios = [
            {'name': 'Without SMOTE', 'smote': False},
            {'name': 'With SMOTE', 'smote': True}
        ]

        for scenario in scenarios:
            print(f"\nEvaluating scenario: {scenario['name']}")

            try:
                # Prepare dataset
                X, y = self.prepare_dataset(with_smote=scenario['smote'])

                # Evaluate each model
                scenario_results = {}
                for model_name in self.models.keys():
                    print(f"Evaluating {model_name}...")
                    metrics = self.evaluate_model(X, y, model_name)
                    scenario_results[model_name] = metrics
                    print(f"{model_name} metrics: {metrics}")

                results[scenario['name']] = scenario_results

            except Exception as e:
                print(f"Error in scenario {scenario['name']}: {e}")
                import traceback
                traceback.print_exc()
                results[scenario['name']] = {'error': str(e)}

        # Create comprehensive results DataFrame
        comparison_data = []
        for scenario, model_results in results.items():
            if 'error' in model_results:
                print(f"Skipping visualization for {scenario} due to error")
                continue

            for model_name, metrics in model_results.items():
                comparison_data.append({
                    'Scenario': scenario,
                    'Model': model_name,
                    'Precision': metrics['precision'],
                    'Recall': metrics['recall'],
                    'F1 Score': metrics['f1_score'],
                    'mAP': metrics['mAP']
                })

        # Create DataFrame if we have data
        if comparison_data:
            comparison_df = pd.DataFrame(comparison_data)
            # Visualize results
            self._visualize_results(comparison_df)
            return comparison_df
        else:
            print("No valid comparison data available.")
            return pd.DataFrame()

    def _visualize_results(self, comparison_df):
        """
        Visualize comparative results

        :param comparison_df: Comparison results DataFrame
        """
        plt.figure(figsize=(15, 10))

        # Metrics to plot
        metrics = ['Precision', 'Recall', 'F1 Score', 'mAP']

        for i, metric in enumerate(metrics, 1):
            plt.subplot(2, 2, i)
            sns.barplot(
                x='Model',
                y=metric,
                hue='Scenario',
                data=comparison_df
            )
            plt.title(f'{metric} Comparison')
            plt.xticks(rotation=45)

        plt.tight_layout()
        plt.savefig('model_comparison.png')
        plt.close()

def main():
    # Import required dependencies for model implementations
    import os
    import sys
    import yaml
    import shutil
    import numpy as np
    import pandas as pd
    import cv2
    import matplotlib.pyplot as plt
    import seaborn as sns
    from sklearn.preprocessing import StandardScaler
    from sklearn.model_selection import StratifiedKFold
    from sklearn.metrics import precision_score, recall_score, f1_score, average_precision_score
    from imblearn.over_sampling import SMOTE
    import torch
    from torch.utils.data import Dataset, DataLoader
    import albumentations as A

    # Import deep learning libraries
    from tensorflow.keras.models import Sequential, Model
    from tensorflow.keras.layers import Dense, Conv2D, Flatten, Dropout, BatchNormalization
    from tensorflow.keras.layers import MaxPooling2D, LeakyReLU, Input, Concatenate, GlobalAveragePooling2D
    from tensorflow.keras import applications

    # Update these paths with your actual directory paths
    base_images_dir = "/path/to/your/local/directory/WAID/images"
    base_labels_dir = "/path/to/your/local/directory/WAID/labels"
    output_dir = "/path/to"

    try:
        print("Initializing Wildlife Dataset Organizer...")
        # Initialize and organize dataset
        organizer = WildlifeDatasetOrganizer(base_images_dir, base_labels_dir, output_dir)

        print("Organizing dataset...")
        # Organize dataset
        organizer.organize_dataset()

        print("Analyzing dataset...")
        # Analyze dataset
        organizer.analyze_dataset()

        print("\nInitializing Wildlife Detection Pipeline...")
        # Initialize detection pipeline
        pipeline = WildlifeDetectionPipeline(output_dir)

        print("Training and evaluating models...")
        # Train and evaluate models
        pipeline.train_and_evaluate_models()

        print("\nInitializing Model Comparison Pipeline...")
        # Initialize comparison pipeline
        comparison = ModelComparisonPipeline(output_dir)

        print("Comparing models...")
        # Compare models
        results = comparison.compare_models()

        print("\nModel comparison complete!")
        print(results)

    except Exception as e:
        print(f"An error occurred in main execution: {e}")
        import traceback
        traceback.print_exc()

if __name__ == '__main__':
    main()


import os
import yaml
import shutil
import numpy as np
import pandas as pd
import cv2
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import precision_score, recall_score, f1_score, average_precision_score
from imblearn.over_sampling import SMOTE
import torch
from torch.utils.data import Dataset, DataLoader
import albumentations as A

# Import deep learning libraries
import tensorflow as tf
from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.layers import Dense, Conv2D, Flatten, Dropout, BatchNormalization
from tensorflow.keras.layers import MaxPooling2D, LeakyReLU, Input, Concatenate, GlobalAveragePooling2D
from tensorflow.keras import applications

class WildlifeDatasetOrganizer:
    def __init__(self, base_images_dir, base_labels_dir, output_dir):
        """
        Initialize dataset organizer

        :param base_images_dir: Base directory containing train/test/valid image folders
        :param base_labels_dir: Base directory containing train/test/valid label folders
        :param output_dir: Output directory for organized dataset
        """
        self.base_images_dir = base_images_dir
        self.base_labels_dir = base_labels_dir
        self.output_dir = output_dir

        # Predefined classes
        self.classes = ['sheep', 'cattle', 'seal', 'camelus', 'kiang', 'zebra']
        self.class_to_index = {cls: idx for idx, cls in enumerate(self.classes)}

        # Create output directories
        self._create_output_directories()

    def _create_output_directories(self):
        """Create output directories for YOLO format"""
        splits = ['train', 'val', 'test']
        for split in splits:
            os.makedirs(os.path.join(self.output_dir, 'images', split), exist_ok=True)
            os.makedirs(os.path.join(self.output_dir, 'labels', split), exist_ok=True)

    def organize_dataset(self):
        """
        Organize dataset into YOLO format
        """
        splits = ['train', 'test', 'val']

        for split in splits:
            # Paths for current split
            images_split_dir = os.path.join(self.base_images_dir, split)
            labels_split_dir = os.path.join(self.base_labels_dir, split)

            # Skip if directory doesn't exist
            if not os.path.exists(images_split_dir) or not os.path.exists(labels_split_dir):
                print(f"Warning: {split} directory not found. Skipping.")
                continue

            # Output paths
            output_images_dir = os.path.join(self.output_dir, 'images', split)
            output_labels_dir = os.path.join(self.output_dir, 'labels', split)

            # Process each image in the split
            for image_filename in os.listdir(images_split_dir):
                if not image_filename.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.tiff')):
                    continue

                # Construct full paths
                image_path = os.path.join(images_split_dir, image_filename)
                base_filename = os.path.splitext(image_filename)[0]

                # Find corresponding label file
                label_filename = base_filename + '.txt'
                label_path = os.path.join(labels_split_dir, label_filename)

                # Skip if no corresponding label file
                if not os.path.exists(label_path):
                    print(f"Warning: No label file found for {image_filename}")
                    continue

                # Copy image to output directory
                shutil.copy(image_path, os.path.join(output_images_dir, image_filename))

                # Copy label file to output directory
                shutil.copy(label_path, os.path.join(output_labels_dir, label_filename))

            print(f"Processed {split} split")

        # Create YOLO dataset configuration
        self._create_dataset_yaml()

    def _create_dataset_yaml(self):
        """
        Create YAML configuration file for YOLO
        """
        data = {
            'train': os.path.join(self.output_dir, 'images', 'train'),
            'val': os.path.join(self.output_dir, 'images', 'val'),
            'test': os.path.join(self.output_dir, 'images', 'test'),
            'nc': len(self.classes),
            'names': self.classes
        }

        yaml_path = os.path.join(self.output_dir, 'wildlife.yaml')
        with open(yaml_path, 'w') as f:
            yaml.dump(data, f, default_flow_style=False)

        print(f"Dataset YAML created at {yaml_path}")

    def analyze_dataset(self):
        """
        Analyze dataset composition
        """
        splits = ['train', 'test', 'val']
        dataset_stats = {}

        for split in splits:
            images_dir = os.path.join(self.output_dir, 'images', split)
            labels_dir = os.path.join(self.output_dir, 'labels', split)

            # Skip if directory doesn't exist
            if not os.path.exists(images_dir) or not os.path.exists(labels_dir):
                print(f"Warning: {split} directory not found. Skipping analysis.")
                continue

            # Count images and annotations
            images = [f for f in os.listdir(images_dir) if f.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.tiff'))]
            labels = [f for f in os.listdir(labels_dir) if f.endswith('.txt')]

            # Analyze class distribution
            class_counts = {cls: 0 for cls in self.classes}
            for label_file in labels:
                with open(os.path.join(labels_dir, label_file), 'r') as f:
                    for line in f:
                        try:
                            class_index = int(line.split()[0])
                            if 0 <= class_index < len(self.classes):  # Added bounds check
                                class_counts[self.classes[class_index]] += 1
                            else:
                                print(f"Warning: Invalid class index {class_index} in {label_file}")
                        except (IndexError, ValueError) as e:
                            print(f"Warning: Invalid label format in {label_file}: {e}")

            dataset_stats[split] = {
                'total_images': len(images),
                'total_labels': len(labels),
                'class_distribution': class_counts
            }

        # Print dataset statistics
        print("\nDataset Statistics:")
        for split, stats in dataset_stats.items():
            print(f"\n{split.upper()} Split:")
            print(f"Total Images: {stats['total_images']}")
            print(f"Total Labeled Objects: {sum(stats['class_distribution'].values())}")
            print("Class Distribution:")
            for cls, count in stats['class_distribution'].items():
                print(f"  {cls}: {count}")

        return dataset_stats

class WildlifeDataset(Dataset):
    def __init__(self, images_dir, labels_dir, transform=None):
        """
        Custom PyTorch Dataset for wildlife images

        :param images_dir: Directory containing images
        :param labels_dir: Directory containing label files
        :param transform: Optional image transformations
        """
        self.images_dir = images_dir
        self.labels_dir = labels_dir
        self.transform = transform

        # Find all image files
        self.image_files = [f for f in os.listdir(images_dir)
                            if f.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp'))]

    def __len__(self):
        return len(self.image_files)

    def __getitem__(self, idx):
        # Get image filename
        img_filename = self.image_files[idx]
        img_path = os.path.join(self.images_dir, img_filename)

        # Read image
        img = cv2.imread(img_path)
        if img is None:
            raise ValueError(f"Failed to load image: {img_path}")

        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

        # Resize with consistent method
        img = cv2.resize(img, (640, 640), interpolation=cv2.INTER_AREA)

        # Convert to float and normalize
        img = img.astype(np.float32) / 255.0

        # Apply transformations if any
        if self.transform:
            transformed = self.transform(image=img)
            img = transformed['image']

        # Convert to tensor with explicit dtype
        img_tensor = torch.from_numpy(img).permute(2, 0, 1).float()

        # Read corresponding label
        label_filename = os.path.splitext(img_filename)[0] + '.txt'
        label_path = os.path.join(self.labels_dir, label_filename)

        # Read labels with error handling
        labels = torch.tensor([], dtype=torch.float32)
        if os.path.exists(label_path):
            try:
                with open(label_path, 'r') as f:
                    labels_list = [list(map(float, line.strip().split())) for line in f]
                if labels_list:  # Check if the list is not empty
                    labels = torch.tensor(labels_list, dtype=torch.float32)
            except Exception as e:
                print(f"Error reading labels for {img_filename}: {e}")

        return img_tensor, labels

class WildlifeDetectionPipeline:
    def __init__(self, dataset_path, batch_size=8, num_workers=4):
        """
        Initialize Wildlife Detection Pipeline

        :param dataset_path: Path to prepared YOLO dataset
        :param batch_size: Batch size for data loading
        :param num_workers: Number of workers for data loading
        """
        self.dataset_path = dataset_path
        self.batch_size = batch_size
        self.num_workers = num_workers

        # Align with the first script's class list
        self.classes = ['sheep', 'cattle', 'seal', 'camelus', 'kiang', 'zebra']

        # Load dataset configuration
        yaml_path = os.path.join(self.dataset_path, 'wildlife.yaml')
        if not os.path.exists(yaml_path):
            raise FileNotFoundError(f"Dataset configuration file not found at {yaml_path}")

        with open(yaml_path, 'r') as f:
            self.dataset_config = yaml.safe_load(f)

    def _create_dataloader(self, images_dir, labels_dir, shuffle=True):
        """
        Create a DataLoader for a specific dataset split

        :param images_dir: Directory containing images
        :param labels_dir: Directory containing labels
        :param shuffle: Whether to shuffle the data
        :return: PyTorch DataLoader
        """
        # Check if directories exist
        if not os.path.exists(images_dir):
            raise FileNotFoundError(f"Images directory not found: {images_dir}")
        if not os.path.exists(labels_dir):
            raise FileNotFoundError(f"Labels directory not found: {labels_dir}")

        # Basic augmentation with Albumentations
        transform = A.Compose([
            A.RandomCrop(width=640, height=640),
            A.HorizontalFlip(p=0.5),
            A.RandomBrightnessContrast(p=0.2),
        ])

        dataset = WildlifeDataset(images_dir, labels_dir, transform=transform)

        # Custom collate function to handle variable-length labels
        def custom_collate(batch):
            images = torch.stack([item[0] for item in batch])

            # Handle labels
            labels = [item[1] for item in batch]

            # Return images and original (non-padded) labels
            return images, labels

        return DataLoader(
            dataset,
            batch_size=self.batch_size,
            shuffle=shuffle,
            num_workers=self.num_workers,
            pin_memory=True,
            collate_fn=custom_collate
        )

    def train_and_evaluate_models(self):
        """
        Train and evaluate models using memory-efficient approach

        :return: Performance metrics
        """
        # Create dataloaders
        try:
            # Corrected paths: labels directory must be specified correctly
            train_images_dir = self.dataset_config['train']
            train_labels_dir = os.path.join(self.dataset_path, 'labels', 'train')

            val_images_dir = self.dataset_config['val']
            val_labels_dir = os.path.join(self.dataset_path, 'labels', 'val')

            train_loader = self._create_dataloader(train_images_dir, train_labels_dir)
            val_loader = self._create_dataloader(val_images_dir, val_labels_dir, shuffle=False)

            # Process and log dataset information
            print(f"Training samples: {len(train_loader.dataset)}")
            print(f"Validation samples: {len(val_loader.dataset)}")

            # Simulate model training with memory-safe iterations
            self._simulate_training(train_loader, val_loader)

        except Exception as e:
            print(f"Error in data loading: {e}")
            import traceback
            traceback.print_exc()

    def _simulate_training(self, train_loader, val_loader):
        """
        Simulate training process with memory-safe iterations

        :param train_loader: Training data loader
        :param val_loader: Validation data loader
        """
        # Basic training simulation
        for epoch in range(3):  # Simulate 3 epochs
            print(f"\nEpoch {epoch + 1}")

            # Training phase
            for batch_idx, (images, labels) in enumerate(train_loader):
                # Move to device if using GPU
                images = images.float()

                # Print batch information
                print(f"Batch {batch_idx + 1}/{len(train_loader)}")
                print(f"  Images shape: {images.shape}")
                print(f"  Number of labels: {len(labels)}")

                # Clear GPU cache periodically
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()

                # Break after a few batches to prevent long-running simulation
                if batch_idx >= 5:
                    break

class ModelComparisonPipeline:
    def __init__(self, dataset_path):
        """
        Initialize Model Comparison Pipeline

        :param dataset_path: Path to prepared YOLO dataset
        """
        self.dataset_path = dataset_path
        self.classes = ['sheep', 'cattle', 'seal', 'camelus', 'kiang', 'zebra']

    def prepare_dataset(self, with_smote=False):
        """
        Prepare dataset with optional SMOTE oversampling

        :param with_smote: Whether to apply SMOTE
        :return: Prepared dataset
        """
        # Load images and labels
        train_images_dir = os.path.join(self.dataset_path, 'images', 'train')
        train_labels_dir = os.path.join(self.dataset_path, 'labels', 'train')

        # Check if directories exist
        if not os.path.exists(train_images_dir):
            raise FileNotFoundError(f"Images directory not found: {train_images_dir}")
        if not os.path.exists(train_labels_dir):
            raise FileNotFoundError(f"Labels directory not found: {train_labels_dir}")

        # Collect image features and labels
        X, y = [], []

        # Increase the number of samples for better SMOTE application
        max_samples = 500
        sample_count = 0

        for img_file in os.listdir(train_images_dir):
            if not img_file.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp')):
                continue

            if sample_count >= max_samples:
                break

            try:
                # Read image
                img_path = os.path.join(train_images_dir, img_file)
                img = cv2.imread(img_path)
                if img is None:
                    print(f"Warning: Could not load image {img_path}")
                    continue

                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
                img = cv2.resize(img, (224, 224))

                # Flatten and normalize image (use reduced dimensions for memory efficiency)
                img_reduced = cv2.resize(img, (64, 64))  # Reduced size
                img_flat = img_reduced.flatten().astype(np.float32)

                # Read corresponding label
                label_file = os.path.splitext(img_file)[0] + '.txt'
                label_path = os.path.join(train_labels_dir, label_file)

                if not os.path.exists(label_path):
                    continue

                # Extract class from first line of label file
                with open(label_path, 'r') as f:
                    first_line = f.readline().strip().split()
                    if not first_line:
                        continue
                    class_index = int(first_line[0])

                    # Validate class index
                    if class_index < 0 or class_index >= len(self.classes):
                        print(f"Warning: Invalid class index {class_index} in {label_file}")
                        continue

                # Add to dataset
                X.append(img_flat)
                y.append(class_index)
                sample_count += 1

            except Exception as e:
                print(f"Error processing {img_file}: {e}")

        # Check if we have enough data
        if len(X) < 10:
            raise ValueError("Not enough valid samples found. Check your dataset.")

        # Convert to numpy arrays
        X = np.array(X)
        y = np.array(y)

        # Normalize features
        scaler = StandardScaler()
        X = scaler.fit_transform(X)

        # Apply SMOTE if requested and if we have enough samples of each class
        if with_smote:
            try:
                # Count samples per class
                class_counts = np.bincount(y)
                print(f"Class distribution before SMOTE: {class_counts}")

                # Reduce n_neighbors for SMOTE to work with small classes
                min_samples = min(class_counts[class_counts > 0])
                if min_samples < 6:
                    k_neighbors = min(min_samples - 1, 5)  # Must be less than min_samples
                    if k_neighbors < 1:
                        print("Not enough samples in minority class for SMOTE, skipping SMOTE")
                    else:
                        print(f"Using k_neighbors={k_neighbors} for SMOTE")
                        smote = SMOTE(random_state=42, k_neighbors=k_neighbors)
                        X, y = smote.fit_resample(X, y)
                        print(f"Class distribution after SMOTE: {np.bincount(y)}")
                else:
                    # Use default SMOTE
                    smote = SMOTE(random_state=42)
                    X, y = smote.fit_resample(X, y)
                    print(f"Class distribution after SMOTE: {np.bincount(y)}")

            except Exception as e:
                print(f"Error applying SMOTE: {e}. Continuing without SMOTE.")

        return X, y

    def evaluate_model(self, X, y, model_name):
        """
        Evaluate model performance based on the specified model architecture

        :param X: Input features
        :param y: Labels
        :param model_name: Name of the model
        :return: Performance metrics
        """
        # Reduce folds if data is limited
        n_splits = min(5, len(np.unique(y)))
        kf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)

        # Initialize performance metrics
        precisions, recalls, f1_scores, maps = [], [], [], []

        for train_index, test_index in kf.split(X, y):
            # Split data
            X_train, X_test = X[train_index], X[test_index]
            y_train, y_test = y[train_index], y[test_index]

            # Train model based on model_name with proper architecture approach
            if model_name == 'YOLOv5':
                # YOLOv5 approach - Uses CSP backbone and PANet neck
                model = Sequential([
                    # CSP-like feature extraction (simplified version)
                    Conv2D(32, (3, 3), padding='same', input_shape=(64, 64, 3)), # Adjust for flattened input
                    BatchNormalization(),
                    LeakyReLU(alpha=0.1),
                    MaxPooling2D(pool_size=(2, 2)),

                    # Add depth (CSP-inspired)
                    Conv2D(64, (3, 3), padding='same'),
                    BatchNormalization(),
                    LeakyReLU(alpha=0.1),
                    MaxPooling2D(pool_size=(2, 2)),

                    # Flatten for classification
                    Flatten(),
                    Dense(128, activation='relu'),
                    Dropout(0.5),
                    Dense(len(self.classes), activation='softmax')
                ])
                model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

                # Reshape X_train and X_test to match input shape expected by CNN
                X_train_reshaped = X_train.reshape(-1, 64, 64, 3)
                X_test_reshaped = X_test.reshape(-1, 64, 64, 3)

                # Train model
                model.fit(X_train_reshaped, y_train, epochs=5, batch_size=32, verbose=0)

                # Predict
                y_pred_proba = model.predict(X_test_reshaped)
                y_pred = np.argmax(y_pred_proba, axis=1)

            elif model_name == 'YOLOv8':
                # YOLOv8 approach - Uses C2f blocks and improved head
                inputs = Input(shape=(64, 64, 3))

                # First conv block
                x = Conv2D(32, (3, 3), padding='same')(inputs)
                x = BatchNormalization()(x)
                x = LeakyReLU(alpha=0.1)(x)
                x = MaxPooling2D(pool_size=(2, 2))(x)

                # C2f-like block (simplified)
                # Branch 1
                branch_1 = Conv2D(32, (1, 1), padding='same')(x)
                branch_1 = BatchNormalization()(branch_1)
                branch_1 = LeakyReLU(alpha=0.1)(branch_1)

                # Branch 2
                branch_2 = Conv2D(32, (1, 1), padding='same')(x)
                branch_2 = BatchNormalization()(branch_2)
                branch_2 = LeakyReLU(alpha=0.1)(branch_2)
                branch_2 = Conv2D(32, (3, 3), padding='same')(branch_2)
                branch_2 = BatchNormalization()(branch_2)
                branch_2 = LeakyReLU(alpha=0.1)(branch_2)

                # Concat branches (C2f style)
                x = Concatenate()([branch_1, branch_2])
                x = Conv2D(64, (1, 1), padding='same')(x)
                x = BatchNormalization()(x)
                x = LeakyReLU(alpha=0.1)(x)
                x = MaxPooling2D(pool_size=(2, 2))(x)

                # Flatten and classification head
                x = Flatten()(x)
                x = Dense(128, activation='relu')(x)
                x = Dropout(0.5)(x)
                outputs = Dense(len(self.classes), activation='softmax')(x)

                model = Model(inputs=inputs, outputs=outputs)
                model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

                # Reshape X_train and X_test
                X_train_reshaped = X_train.reshape(-1, 64, 64, 3)
                X_test_reshaped = X_test.reshape(-1, 64, 64, 3)

                # Train model
                model.fit(X_train_reshaped, y_train, epochs=5, batch_size=32, verbose=0)

                # Predict
                y_pred_proba = model.predict(X_test_reshaped)
                y_pred = np.argmax(y_pred_proba, axis=1)

            elif model_name == 'RCNN':
                # Faster R-CNN approach - Uses two-stage detection with RPN
                # For simplified implementation, using backbone feature extractor only
                backbone = applications.ResNet50(
                    include_top=False,
                    weights=None,
                    input_shape=(64, 64, 3)
                )

                # Add classification head
                model = Sequential([
                    backbone,
                    GlobalAveragePooling2D(),
                    Dense(256, activation='relu'),
                    Dropout(0.5),
                    Dense(len(self.classes), activation='softmax')
                ])

                model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

                # Reshape X_train and X_test
                X_train_reshaped = X_train.reshape(-1, 64, 64, 3)
                X_test_reshaped = X_test.reshape(-1, 64, 64, 3)

                # Train model
                model.fit(X_train_reshaped, y_train, epochs=5, batch_size=16, verbose=0)

                # Predict
                y_pred_proba = model.predict(X_test_reshaped)
                y_pred = np.argmax(y_pred_proba, axis=1)

            # Calculate metrics
            precisions.append(precision_score(y_test, y_pred, average='weighted', zero_division=0))
            recalls.append(recall_score(y_test, y_pred, average='weighted', zero_division=0))
            f1_scores.append(f1_score(y_test, y_pred, average='weighted', zero_division=0))

            # Create one-hot encoding for y_test for mAP calculation
            y_test_one_hot = np.zeros((len(y_test), len(self.classes)))
            for i, label in enumerate(y_test):
                if 0 <= label < len(self.classes):
                    y_test_one_hot[i, label] = 1

            try:
                map_score = average_precision_score(
                    y_test_one_hot,
                    y_pred_proba,
                    average='weighted'
                )
                maps.append(map_score)
            except Exception as e:
                print(f"Error calculating mAP: {e}")
                maps.append(0)

        # Return average metrics
        return {
            'precision': np.mean(precisions),
            'recall': np.mean(recalls),
            'f1_score': np.mean(f1_scores),
            'mAP': np.mean(maps)
        }

    def compare_models(self):
        """
        Compare models with and without SMOTE

        :return: Comparative performance results
        """
        results = {}

        # Scenarios to evaluate
        scenarios = [
            {'name': 'Without SMOTE', 'smote': False},
            {'name': 'With SMOTE', 'smote': True}
        ]

        for scenario in scenarios:
            print(f"\nEvaluating scenario: {scenario['name']}")

            try:
                # Prepare dataset
                X, y = self.prepare_dataset(with_smote=scenario['smote'])

                # Evaluate each model
                scenario_results = {}
                for model_name in ['YOLOv5', 'YOLOv8', 'RCNN']:  # Explicitly define models here
                    print(f"Evaluating {model_name}...")
                    metrics = self.evaluate_model(X, y, model_name)
                    scenario_results[model_name] = metrics
                    print(f"{model_name} metrics: {metrics}")

                results[scenario['name']] = scenario_results

            except Exception as e:
                print(f"Error in scenario {scenario['name']}: {e}")
                import traceback
                traceback.print_exc()
                results[scenario['name']] = {'error': str(e)}

        # Create comprehensive results DataFrame
        comparison_data = []
        for scenario, model_results in results.items():
            if 'error' in model_results:
                print(f"Skipping visualization for {scenario} due to error")
                continue

            for model_name, metrics in model_results.items():
                comparison_data.append({
                    'Scenario': scenario,
                    'Model': model_name,
                    'Precision': metrics['precision'],
                    'Recall': metrics['recall'],
                    'F1 Score': metrics['f1_score'],
                    'mAP': metrics['mAP']
                })

        # Create DataFrame if we have data
        if comparison_data:
            comparison_df = pd.DataFrame(comparison_data)
            # Visualize results
            self._visualize_results(comparison_df)
            return comparison_df
        else:
            print("No valid comparison data available.")
            return pd.DataFrame()

    def _visualize_results(self, comparison_df):
        """
        Visualize comparative results

        :param comparison_df: Comparison results DataFrame
        """
        # Metrics to plot
        metrics = ['Precision', 'Recall', 'F1 Score', 'mAP']

        # Create subplots for each metric
        fig, axes = plt.subplots(2, 2, figsize=(16, 12))
        axes = axes.flatten()

        # Plot each metric
        for i, metric in enumerate(metrics):
            sns.barplot(x='Model', y=metric, hue='Scenario', data=comparison_df, ax=axes[i])
            axes[i].set_title(f'Comparison of {metric} Across Models')
            axes[i].set_xlabel('Model Architecture')
            axes[i].set_ylabel(metric)
            axes[i].legend(title='Scenario')

            # Add value labels on bars
            for p in axes[i].patches:
                axes[i].annotate(f"{p.get_height():.3f}",
                            (p.get_x() + p.get_width() / 2., p.get_height()),
                            ha = 'center', va = 'bottom',
                            xytext = (0, 5), textcoords = 'offset points')

        plt.tight_layout()
        plt.savefig('model_comparison_results.png')
        plt.show()

        # Also create a heatmap of performance across models and scenarios
        plt.figure(figsize=(15, 10))

        # Create pivot tables for each metric and combine into a single heatmap
        heatmap_data = []

        for metric in metrics:
            pivot = comparison_df.pivot(index='Model', columns='Scenario', values=metric)

            # Add metric name to index
            pivot.index = [f"{idx} ({metric})" for idx in pivot.index]

            heatmap_data.append(pivot)

        # Combine all metrics
        combined_data = pd.concat(heatmap_data)

        # Create heatmap
        plt.figure(figsize=(12, 10))
        sns.heatmap(combined_data, annot=True, cmap='viridis', fmt=".3f", linewidths=.5)
        plt.title('Performance Metrics Across Models and Scenarios')
        plt.tight_layout()
        plt.savefig('performance_heatmap.png')
        plt.show()

        # Create radar chart for comprehensive model comparison
        self._create_radar_chart(comparison_df)

    def _create_radar_chart(self, comparison_df):
        """
        Create radar charts to compare models across multiple metrics

        :param comparison_df: Comparison results DataFrame
        """
        # Get unique models and scenarios
        models = comparison_df['Model'].unique()
        scenarios = comparison_df['Scenario'].unique()

        # Metrics for radar chart
        metrics = ['Precision', 'Recall', 'F1 Score', 'mAP']

        # Create radar charts for each scenario
        for scenario in scenarios:
            # Filter data for current scenario
            scenario_df = comparison_df[comparison_df['Scenario'] == scenario]

            # Create figure
            fig = plt.figure(figsize=(10, 10))

            # Number of variables
            N = len(metrics)

            # What will be the angle of each axis in the plot
            angles = [n / float(N) * 2 * np.pi for n in range(N)]
            angles += angles[:1]  # Close the loop

            # Initialize the subplot
            ax = fig.add_subplot(111, polar=True)

            # Draw one axis per variable and add labels
            plt.xticks(angles[:-1], metrics)

            # Draw ylabels
            ax.set_rlabel_position(0)
            plt.yticks([0.25, 0.5, 0.75], ["0.25", "0.5", "0.75"], color="grey", size=8)
            plt.ylim(0, 1)

            # Plot each model
            for model in models:
                model_data = scenario_df[scenario_df['Model'] == model]
                values = model_data[metrics].values.flatten().tolist()
                values += values[:1]  # Close the loop

                # Plot values
                ax.plot(angles, values, linewidth=2, linestyle='solid', label=model)
                ax.fill(angles, values, alpha=0.1)

            # Add legend
            plt.legend(loc='upper right', bbox_to_anchor=(0.1, 0.1))
            plt.title(f'Model Comparison - {scenario}')
            plt.tight_layout()
            plt.savefig(f'radar_chart_{scenario.replace(" ", "_")}.png')
            plt.show()

class WildlifeDetectionApp:
    def __init__(self, input_dir, output_dir):
        """
        Initialize the Wildlife Detection Application

        :param input_dir: Directory containing input data
        :param output_dir: Directory for output data and results
        """
        self.input_dir = input_dir
        self.output_dir = output_dir

        # Create output directory if it doesn't exist
        os.makedirs(output_dir, exist_ok=True)

        # Define subdirectories
        self.base_images_dir = os.path.join(input_dir, 'images')
        self.base_labels_dir = os.path.join(input_dir, 'labels')
        self.processed_dataset_dir = os.path.join(output_dir, 'processed_dataset')

        # Create subdirectories
        os.makedirs(self.processed_dataset_dir, exist_ok=True)

    def run_full_pipeline(self):
        """
        Run the complete wildlife detection pipeline
        """
        try:
            print("===== Wildlife Detection Pipeline =====")
            print("\n1. Organizing dataset...")
            organizer = WildlifeDatasetOrganizer(
                self.base_images_dir,
                self.base_labels_dir,
                self.processed_dataset_dir
            )
            organizer.organize_dataset()

            print("\n2. Analyzing dataset statistics...")
            dataset_stats = organizer.analyze_dataset()

            print("\n3. Training and evaluating detection models...")
            detection_pipeline = WildlifeDetectionPipeline(self.processed_dataset_dir)
            detection_pipeline.train_and_evaluate_models()

            print("\n4. Comparing model architectures...")
            comparison_pipeline = ModelComparisonPipeline(self.processed_dataset_dir)
            comparison_results = comparison_pipeline.compare_models()

            # Save comparison results
            if not comparison_results.empty:
                comparison_results.to_csv(os.path.join(self.output_dir, 'model_comparison_results.csv'))
                print(f"Results saved to {os.path.join(self.output_dir, 'model_comparison_results.csv')}")

            print("\n===== Pipeline completed successfully =====")

            return {
                'dataset_stats': dataset_stats,
                'comparison_results': comparison_results
            }

        except Exception as e:
            print(f"Error in pipeline: {e}")
            import traceback
            traceback.print_exc()
            return {'error': str(e)}

    def evaluate_on_test_set(self, model_type='YOLOv8'):
        """
        Evaluate the best model on the test set

        :param model_type: Type of model to evaluate ('YOLOv5', 'YOLOv8', or 'RCNN')
        :return: Test set performance metrics
        """
        print(f"\nEvaluating {model_type} on test set...")

        try:
            # Load test set
            test_images_dir = os.path.join(self.processed_dataset_dir, 'images', 'test')
            test_labels_dir = os.path.join(self.processed_dataset_dir, 'labels', 'test')

            # Check if directories exist
            if not os.path.exists(test_images_dir) or not os.path.exists(test_labels_dir):
                print("Test set not found. Skipping evaluation.")
                return None

            # Initialize model comparison pipeline
            comparison_pipeline = ModelComparisonPipeline(self.processed_dataset_dir)

            # Prepare test dataset (without SMOTE as this is test data)
            X_test, y_test = [], []

            # Process test images
            for img_file in os.listdir(test_images_dir):
                if not img_file.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp')):
                    continue

                try:
                    # Read image
                    img_path = os.path.join(test_images_dir, img_file)
                    img = cv2.imread(img_path)
                    if img is None:
                        continue

                    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
                    img = cv2.resize(img, (224, 224))

                    # Reduce dimensions for memory efficiency
                    img_reduced = cv2.resize(img, (64, 64))
                    img_flat = img_reduced.flatten().astype(np.float32)

                    # Read corresponding label
                    label_file = os.path.splitext(img_file)[0] + '.txt'
                    label_path = os.path.join(test_labels_dir, label_file)

                    if not os.path.exists(label_path):
                        continue

                    # Extract class from first line of label file
                    with open(label_path, 'r') as f:
                        first_line = f.readline().strip().split()
                        if not first_line:
                            continue
                        class_index = int(first_line[0])

                    # Add to dataset
                    X_test.append(img_flat)
                    y_test.append(class_index)

                except Exception as e:
                    print(f"Error processing test image {img_file}: {e}")

            # Convert to numpy arrays
            X_test = np.array(X_test)
            y_test = np.array(y_test)

            # Normalize features
            scaler = StandardScaler()
            X_test = scaler.fit_transform(X_test)

            # Train on training data with SMOTE
            X_train, y_train = comparison_pipeline.prepare_dataset(with_smote=True)

            # Evaluate the specified model
            metrics = comparison_pipeline.evaluate_model(
                np.concatenate([X_train, X_test]),
                np.concatenate([y_train, y_test]),
                model_type
            )

            print(f"Test set metrics for {model_type}:")
            for metric, value in metrics.items():
                print(f"  {metric}: {value:.4f}")

            return metrics

        except Exception as e:
            print(f"Error evaluating on test set: {e}")
            import traceback
            traceback.print_exc()
            return {'error': str(e)}

    def visualize_predictions(self, model_type='YOLOv8', num_samples=5):
        """
        Visualize model predictions on random test samples

        :param model_type: Type of model to use ('YOLOv5', 'YOLOv8', or 'RCNN')
        :param num_samples: Number of samples to visualize
        """
        print(f"\nVisualizing {model_type} predictions on {num_samples} test samples...")

        try:
            # Load test set
            test_images_dir = os.path.join(self.processed_dataset_dir, 'images', 'test')
            test_labels_dir = os.path.join(self.processed_dataset_dir, 'labels', 'test')

            if not os.path.exists(test_images_dir) or not os.path.exists(test_labels_dir):
                print("Test set not found. Skipping visualization.")
                return

            # Get list of test images
            test_images = [f for f in os.listdir(test_images_dir)
                          if f.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp'))]

            if len(test_images) == 0:
                print("No test images found. Skipping visualization.")
                return

            # Randomly select samples
            num_samples = min(num_samples, len(test_images))
            selected_images = np.random.choice(test_images, num_samples, replace=False)

            # Classes
            classes = ['sheep', 'cattle', 'seal', 'camelus', 'kiang', 'zebra']

            # Initialize model
            if model_type == 'YOLOv5':
                # YOLOv5 model
                model = Sequential([
                    Conv2D(32, (3, 3), padding='same', input_shape=(64, 64, 3)),
                    BatchNormalization(),
                    LeakyReLU(alpha=0.1),
                    MaxPooling2D(pool_size=(2, 2)),

                    Conv2D(64, (3, 3), padding='same'),
                    BatchNormalization(),
                    LeakyReLU(alpha=0.1),
                    MaxPooling2D(pool_size=(2, 2)),

                    Flatten(),
                    Dense(128, activation='relu'),
                    Dropout(0.5),
                    Dense(len(classes), activation='softmax')
                ])

            elif model_type == 'YOLOv8':
                # YOLOv8 model
                inputs = Input(shape=(64, 64, 3))

                # First conv block
                x = Conv2D(32, (3, 3), padding='same')(inputs)
                x = BatchNormalization()(x)
                x = LeakyReLU(alpha=0.1)(x)
                x = MaxPooling2D(pool_size=(2, 2))(x)

                # C2f-like block (simplified)
                branch_1 = Conv2D(32, (1, 1), padding='same')(x)
                branch_1 = BatchNormalization()(branch_1)
                branch_1 = LeakyReLU(alpha=0.1)(branch_1)

                branch_2 = Conv2D(32, (1, 1), padding='same')(x)
                branch_2 = BatchNormalization()(branch_2)
                branch_2 = LeakyReLU(alpha=0.1)(branch_2)
                branch_2 = Conv2D(32, (3, 3), padding='same')(branch_2)
                branch_2 = BatchNormalization()(branch_2)
                branch_2 = LeakyReLU(alpha=0.1)(branch_2)

                x = Concatenate()([branch_1, branch_2])
                x = Conv2D(64, (1, 1), padding='same')(x)
                x = BatchNormalization()(x)
                x = LeakyReLU(alpha=0.1)(x)
                x = MaxPooling2D(pool_size=(2, 2))(x)

                x = Flatten()(x)
                x = Dense(128, activation='relu')(x)
                x = Dropout(0.5)(x)
                outputs = Dense(len(classes), activation='softmax')(x)

                model = Model(inputs=inputs, outputs=outputs)

            else:  # RCNN
                backbone = applications.ResNet50(
                    include_top=False,
                    weights=None,
                    input_shape=(64, 64, 3)
                )

                model = Sequential([
                    backbone,
                    GlobalAveragePooling2D(),
                    Dense(256, activation='relu'),
                    Dropout(0.5),
                    Dense(len(classes), activation='softmax')
                ])

            model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

            # Train model on a few examples (simplified training)
            X_train, y_train = [], []
            for idx, img_file in enumerate(os.listdir(test_images_dir)):
                if idx >= 50:  # Limit to 50 samples for quick training
                    break

                try:
                    # Read image
                    img_path = os.path.join(test_images_dir, img_file)
                    img = cv2.imread(img_path)
                    if img is None:
                        continue

                    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
                    img = cv2.resize(img, (64, 64))
                    img_norm = img.astype(np.float32) / 255.0

                    # Read label
                    label_file = os.path.splitext(img_file)[0] + '.txt'
                    label_path = os.path.join(test_labels_dir, label_file)

                    if not os.path.exists(label_path):
                        continue

                    with open(label_path, 'r') as f:
                        first_line = f.readline().strip().split()
                        if not first_line:
                            continue
                        class_index = int(first_line[0])

                    X_train.append(img_norm)
                    y_train.append(class_index)

                except Exception as e:
                    print(f"Error processing training image {img_file}: {e}")

            # Convert to numpy arrays
            X_train = np.array(X_train)
            y_train = np.array(y_train)

            # Train model
            if len(X_train) > 0:
                model.fit(X_train, y_train, epochs=5, batch_size=16, verbose=1)

                # Visualize predictions
                plt.figure(figsize=(15, 5 * num_samples))

                for i, image_filename in enumerate(selected_images):
                    try:
                        # Read image
                        img_path = os.path.join(test_images_dir, image_filename)
                        img = cv2.imread(img_path)
                        if img is None:
                            continue

                        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

                        # Original image for display
                        display_img = cv2.resize(img, (640, 640))

                        # Preprocessed image for prediction
                        pred_img = cv2.resize(img, (64, 64))
                        pred_img = pred_img.astype(np.float32) / 255.0

                        # Make prediction
                        prediction = model.predict(np.expand_dims(pred_img, axis=0))
                        predicted_class_index = np.argmax(prediction[0])
                        predicted_class = classes[predicted_class_index]
                        confidence = prediction[0][predicted_class_index]

                        # Read true label
                        label_file = os.path.splitext(image_filename)[0] + '.txt'
                        label_path = os.path.join(test_labels_dir, label_file)
                        true_class = "Unknown"

                        if os.path.exists(label_path):
                            with open(label_path, 'r') as f:
                                first_line = f.readline().strip().split()
                                if first_line:
                                    true_class_index = int(first_line[0])
                                    if 0 <= true_class_index < len(classes):
                                        true_class = classes[true_class_index]

                        # Plot
                        plt.subplot(num_samples, 1, i + 1)
                        plt.imshow(display_img)

                        # Color green if correct, red if wrong
                        color = 'green' if predicted_class == true_class else 'red'
                        plt.title(f"True: {true_class}, Predicted: {predicted_class} ({confidence:.2f})",
                                color=color, fontsize=14)
                        plt.axis('off')

                    except Exception as e:
                        print(f"Error visualizing prediction for {image_filename}: {e}")

                plt.tight_layout()
                plt.savefig(os.path.join(self.output_dir, f'{model_type}_predictions.png'))
                plt.show()

            else:
                print("Not enough training samples for visualization.")

        except Exception as e:
            print(f"Error in visualization: {e}")
            import traceback
            traceback.print_exc()

# Main execution
if __name__ == "__main__":
    # Define input and output directories
    input_dir = "/path/to/your/local/directory/WAID"
    output_dir = "wildlife_detection_results"

    # Create and run the application
    app = WildlifeDetectionApp(input_dir, output_dir)

    # Run the full pipeline
    results = app.run_full_pipeline()

    # Evaluate the best model on test set
    test_metrics = app.evaluate_on_test_set(model_type='YOLOv8')

    # Visualize some predictions
    app.visualize_predictions(model_type='YOLOv8', num_samples=3)

import streamlit as st
import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
import yaml
import shutil
import cv2
from PIL import Image
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
from streamlit_mermaid import st_mermaid

st.set_page_config(
    page_title="Wildlife Detection Pipeline",
    page_icon="🦓",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Custom CSS for better styling
st.markdown("""
<style>
    .main-header {
        font-size: 2.5rem;
        color: #2E8B57;
        text-align: center;
        margin-bottom: 2rem;
    }
    .section-header {
        font-size: 1.5rem;
        color: #4682B4;
        margin-top: 2rem;
        margin-bottom: 1rem;
    }
    .metric-card {
        background-color: #f0f2f6;
        padding: 1rem;
        border-radius: 10px;
        margin: 0.5rem;
    }
</style>
""", unsafe_allow_html=True)

# Main title
st.markdown('<h1 class="main-header">🦓 Wildlife Detection Pipeline</h1>', unsafe_allow_html=True)

# Sidebar navigation
st.sidebar.title("Navigation")
page = st.sidebar.selectbox(
    "Choose a page",
    ["Home", "Dataset Organization", "Model Training", "Model Comparison", "Results Analysis"]
)

# Initialize session state
if 'dataset_organized' not in st.session_state:
    st.session_state.dataset_organized = False
if 'models_trained' not in st.session_state:
    st.session_state.models_trained = False
if 'comparison_results' not in st.session_state:
    st.session_state.comparison_results = None

def home_page():
    st.markdown("## Welcome to the Wildlife Detection Pipeline!")

    col1, col2 = st.columns(2)

    with col1:
        st.markdown("### 🎯 Features")
        st.markdown("""
        - **Dataset Organization**: Convert your wildlife datasets to YOLO format
        - **Model Training**: Train multiple object detection models
        - **Model Comparison**: Compare YOLOv5, YOLOv8, and R-CNN performance
        - **SMOTE Integration**: Handle class imbalance with SMOTE oversampling
        - **Interactive Analysis**: Visualize results and metrics
        """)

    with col2:
        st.markdown("### 🦎 Supported Animals")
        animals = ['Sheep', 'Cattle', 'Seal', 'Camelus', 'Kiang', 'Zebra']
        for animal in animals:
            st.markdown(f"- {animal}")

    st.markdown("### 📊 Pipeline Overview")
    # Changed from st.mermaid to st_mermaid
    st_mermaid("""
    graph TD
        A[Upload Dataset] --> B[Dataset Organization]
        B --> C[Data Analysis]
        C --> D[Model Training]
        D --> E[Model Comparison]
        E --> F[Results Visualization]
    """)

def dataset_organization_page():
    st.markdown('<h2 class="section-header">📁 Dataset Organization</h2>', unsafe_allow_html=True)

    # Path configuration
    st.markdown("### Configuration")
    col1, col2, col3 = st.columns(3)

    with col1:
        base_images_dir = st.text_input(
            "Base Images Directory",
            value="/path/to/your/local/directory/WAID/images",
            help="Directory containing train/test/valid image folders"
        )

    with col2:
        base_labels_dir = st.text_input(
            "Base Labels Directory",
            value="/path/to/your/local/directory/WAID/labels",
            help="Directory containing train/test/valid label folders"
        )

    with col3:
        output_dir = st.text_input(
            "Output Directory",
            value="/path/to/output",
            help="Directory for organized dataset"
        )

    # Dataset organization
    if st.button("🔄 Organize Dataset", type="primary"):
        if all([base_images_dir, base_labels_dir, output_dir]):
            with st.spinner("Organizing dataset..."):
                try:
                    # This would use your WildlifeDatasetOrganizer class
                    # organizer = WildlifeDatasetOrganizer(base_images_dir, base_labels_dir, output_dir)
                    # organizer.organize_dataset()
                    # stats = organizer.analyze_dataset()

                    # Simulated results for demo
                    st.success("Dataset organized successfully!")
                    st.session_state.dataset_organized = True

                    # Display sample statistics
                    sample_stats = {
                        'train': {'total_images': 1200, 'total_labels': 1200},
                        'val': {'total_images': 300, 'total_labels': 300},
                        'test': {'total_images': 400, 'total_labels': 400}
                    }

                    st.markdown("### 📊 Dataset Statistics")
                    for split, stats in sample_stats.items():
                        col1, col2 = st.columns(2)
                        with col1:
                            st.metric(f"{split.upper()} Images", stats['total_images'])
                        with col2:
                            st.metric(f"{split.upper()} Labels", stats['total_labels'])

                except Exception as e:
                    st.error(f"Error organizing dataset: {str(e)}")
        else:
            st.error("Please fill in all directory paths")

    # Sample images display
    if st.session_state.dataset_organized:
        st.markdown("### 🖼️ Sample Images")
        # This would display actual sample images from your dataset
        st.info("Sample images would be displayed here from your organized dataset")

def model_training_page():
    st.markdown('<h2 class="section-header">🚀 Model Training</h2>', unsafe_allow_html=True)

    if not st.session_state.dataset_organized:
        st.warning("Please organize your dataset first!")
        return

    # Training configuration
    st.markdown("### Training Configuration")
    col1, col2, col3 = st.columns(3)

    with col1:
        batch_size = st.selectbox("Batch Size", [4, 8, 16, 32], index=1)

    with col2:
        num_workers = st.selectbox("Number of Workers", [2, 4, 8], index=1)

    with col3:
        epochs = st.selectbox("Epochs", [5, 10, 20, 50], index=0)

    # Model selection
    st.markdown("### Model Selection")
    models_to_train = st.multiselect(
        "Select models to train",
        ["YOLOv5", "YOLOv8", "R-CNN"],
        default=["YOLOv5", "YOLOv8"]
    )

    # Training button
    if st.button("🎯 Start Training", type="primary"):
        if models_to_train:
            progress_bar = st.progress(0)
            status_text = st.empty()

            for i, model in enumerate(models_to_train):
                status_text.text(f"Training {model}...")

                # Simulate training progress
                for j in range(epochs):
                    progress_bar.progress((i * epochs + j + 1) / (len(models_to_train) * epochs))
                    st.write(f"Epoch {j+1}/{epochs} - {model}")

            st.success("Training completed!")
            st.session_state.models_trained = True
        else:
            st.error("Please select at least one model to train")

def model_comparison_page():
    st.markdown('<h2 class="section-header">⚖️ Model Comparison</h2>', unsafe_allow_html=True)

    if not st.session_state.models_trained:
        st.warning("Please train models first!")
        return

    # Comparison options
    st.markdown("### Comparison Options")
    col1, col2 = st.columns(2)

    with col1:
        use_smote = st.checkbox("Use SMOTE for class imbalance", value=True)

    with col2:
        cross_validation = st.checkbox("Use Cross-Validation", value=True)

    # Run comparison
    if st.button("📊 Compare Models", type="primary"):
        with st.spinner("Comparing models..."):
            # Simulate comparison results
            comparison_data = {
                'Model': ['YOLOv5', 'YOLOv8', 'R-CNN'] * 2,
                'Scenario': ['Without SMOTE'] * 3 + ['With SMOTE'] * 3,
                'Precision': [0.85, 0.88, 0.82, 0.87, 0.90, 0.84],
                'Recall': [0.82, 0.85, 0.79, 0.86, 0.88, 0.83],
                'F1 Score': [0.83, 0.86, 0.80, 0.86, 0.89, 0.83],
                'mAP': [0.81, 0.84, 0.77, 0.85, 0.87, 0.82]
            }

            comparison_df = pd.DataFrame(comparison_data)
            st.session_state.comparison_results = comparison_df

            st.success("Model comparison completed!")

            # Display results table
            st.markdown("### 📈 Results Table")
            st.dataframe(comparison_df, use_container_width=True)

def results_analysis_page():
    st.markdown('<h2 class="section-header">📊 Results Analysis</h2>', unsafe_allow_html=True)

    if st.session_state.comparison_results is None:
        st.warning("Please run model comparison first!")
        return

    df = st.session_state.comparison_results

    # Metrics overview
    st.markdown("### 🎯 Performance Metrics")
    metrics = ['Precision', 'Recall', 'F1 Score', 'mAP']

    # Create tabs for different visualizations
    tab1, tab2, tab3, tab4 = st.tabs(["📊 Bar Charts", "📈 Line Charts", "🎯 Radar Chart", "📋 Summary"])

    with tab1:
        # Bar charts for each metric
        for metric in metrics:
            fig = px.bar(
                df,
                x='Model',
                y=metric,
                color='Scenario',
                title=f'{metric} Comparison',
                barmode='group'
            )
            st.plotly_chart(fig, use_container_width=True)

    with tab2:
        # Line chart showing all metrics
        fig = make_subplots(
            rows=2, cols=2,
            subplot_titles=metrics,
            specs=[[{"secondary_y": False}, {"secondary_y": False}],
                   [{"secondary_y": False}, {"secondary_y": False}]]
        )

        for i, metric in enumerate(metrics):
            row = i // 2 + 1
            col = i % 2 + 1

            for scenario in df['Scenario'].unique():
                scenario_data = df[df['Scenario'] == scenario]
                fig.add_trace(
                    go.Scatter(
                        x=scenario_data['Model'],
                        y=scenario_data[metric],
                        name=f'{scenario} - {metric}',
                        mode='lines+markers'
                    ),
                    row=row, col=col
                )

        fig.update_layout(height=600, title_text="Performance Metrics Comparison")
        st.plotly_chart(fig, use_container_width=True)

    with tab3:
        # Radar chart for model comparison
        st.markdown("### 🎯 Radar Chart - Model Performance")

        # Select scenario for radar chart
        scenario = st.selectbox("Select Scenario", df['Scenario'].unique())
        scenario_data = df[df['Scenario'] == scenario]

        fig = go.Figure()

        for model in scenario_data['Model'].unique():
            model_data = scenario_data[scenario_data['Model'] == model]

            fig.add_trace(go.Scatterpolar(
                r=[model_data[metric].values[0] for metric in metrics],
                theta=metrics,
                fill='toself',
                name=model
            ))

        fig.update_layout(
            polar=dict(
                radialaxis=dict(
                    visible=True,
                    range=[0, 1]
                )
            ),
            showlegend=True,
            title=f"Model Performance Radar Chart - {scenario}"
        )

        st.plotly_chart(fig, use_container_width=True)

    with tab4:
        # Summary statistics
        st.markdown("### 📋 Summary Statistics")

        # Best performing model per metric
        st.markdown("#### 🏆 Best Performing Models")
        for metric in metrics:
            best_row = df.loc[df[metric].idxmax()]
            st.markdown(f"**{metric}**: {best_row['Model']} ({best_row['Scenario']}) - {best_row[metric]:.3f}")

        # Average performance by scenario
        st.markdown("#### 📊 Average Performance by Scenario")
        avg_by_scenario = df.groupby('Scenario')[metrics].mean()
        st.dataframe(avg_by_scenario)

        # Model ranking
        st.markdown("#### 🥇 Overall Model Ranking")
        df['Overall_Score'] = df[metrics].mean(axis=1)
        ranking = df.nlargest(6, 'Overall_Score')[['Model', 'Scenario', 'Overall_Score']]
        st.dataframe(ranking)

# Page routing
if page == "Home":
    home_page()
elif page == "Dataset Organization":
    dataset_organization_page()
elif page == "Model Training":
    model_training_page()
elif page == "Model Comparison":
    model_comparison_page()
elif page == "Results Analysis":
    results_analysis_page()

# Footer
st.markdown("---")
st.markdown("Built with ❤️ using Streamlit | Wildlife Detection Pipeline v1.0")

# Save this as launcher.py
import subprocess
import sys

def run_streamlit():
    subprocess.run([sys.executable, "-m", "streamlit", "run", "wildlife_app.py"])

if __name__ == "__main__":
    run_streamlit()

import subprocess
subprocess.run(["streamlit", "run", "wildlife_app.py"])
